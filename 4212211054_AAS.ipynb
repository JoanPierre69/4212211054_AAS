{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOh//k8zA1J24E+/9rSDKU3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JoanPierre69/4212211054_AAS/blob/main/4212211054_AAS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 185,
      "metadata": {
        "id": "uoGZ_TZa5sg0"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import numpy as np\n",
        "from torchvision import transforms, models\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch import nn, optim\n",
        "from PIL import Image\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, f1_score\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " train_data = pd.read_csv('/content/emnist-bymerge-train.csv', header=None, nrows=1000)\n",
        " test_data = pd.read_csv('/content/emnist-bymerge-test.csv', header=None, nrows=1000)"
      ],
      "metadata": {
        "id": "AFDqz3SK6Gj0"
      },
      "execution_count": 186,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_image(data):\n",
        "    data = np.clip(data, 0, 255).astype(np.uint8).reshape(28, 28)\n",
        "    return Image.fromarray(data).convert(\"RGB\")"
      ],
      "metadata": {
        "id": "SBGIvZXN6PFE"
      },
      "execution_count": 187,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " class CustomDataset(Dataset):\n",
        "    def __init__(self, dataframe, transform=None):\n",
        "        \"\"\"\n",
        "        Initialize the dataset with a DataFrame and optional image transformations.\n",
        "        \"\"\"\n",
        "        self.dataframe = dataframe\n",
        "        self.transform = transform\n",
        "    def __len__(self):\n",
        "        \"\"\"\n",
        "        Returns the total number of samples in the dataset.\n",
        "        \"\"\"\n",
        "        return len(self.dataframe)\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        Retrieves the image and label for a given index, applies preprocessing and transformations.\n",
        "        \"\"\"\n",
        "        label = self.dataframe.iloc[idx, 0]\n",
        "        img_data = self.dataframe.iloc[idx, 1:].values\n",
        "        image = preprocess_image(img_data)\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image, label\n",
        "    transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor()\n",
        "    ])"
      ],
      "metadata": {
        "id": "WiQeFRp16SIT"
      },
      "execution_count": 188,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " # Create datasets and data loaders for training and validation\n",
        " # Use train_data and test_data instead of data_train and data_val\n",
        " train_dataset = CustomDataset(train_data, transform=CustomDataset.transform)  # Use CustomDataset.transform\n",
        " val_dataset = CustomDataset(test_data, transform=CustomDataset.transform)  # Use CustomDataset.transform\n",
        " train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)  # Training data loader\n",
        " val_loader = DataLoader(val_dataset, batch_size=4, shuffle=False)    # Validation data loader\n"
      ],
      "metadata": {
        "id": "xN47apqN6VQv"
      },
      "execution_count": 189,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " # Initialize a pretrained AlexNet model for transfer learning\n",
        " model = models.alexnet(pretrained=True)\n",
        " model.classifier[6] = nn.Linear(4096, 200)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9yuzy7Y5ExAF",
        "outputId": "57fee235-e1be-47a2-b370-c835c5cee552"
      },
      "execution_count": 190,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Freeze feature extraction layers to only train the classifier\n",
        "for param in model.features.parameters():\n",
        "    param.requires_grad = False"
      ],
      "metadata": {
        "id": "YfEnZ-937igT"
      },
      "execution_count": 191,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " # Define loss function and optimizer\n",
        " criterion = nn.CrossEntropyLoss()\n",
        " optimizer = optim.Adam(model.parameters(), lr=0.0001)"
      ],
      "metadata": {
        "id": "FK0ec2NH7ml6"
      },
      "execution_count": 192,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " # Configure the device for GPU acceleration (if available)\n",
        " device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        " model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4-oUaxR77plv",
        "outputId": "7fcf9f67-46f6-40b9-9e4d-9bb00d4d5b8c"
      },
      "execution_count": 193,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AlexNet(\n",
              "  (features): Sequential(\n",
              "    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
              "    (1): ReLU(inplace=True)\n",
              "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
              "    (4): ReLU(inplace=True)\n",
              "    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (7): ReLU(inplace=True)\n",
              "    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (9): ReLU(inplace=True)\n",
              "    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (11): ReLU(inplace=True)\n",
              "    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(6, 6))\n",
              "  (classifier): Sequential(\n",
              "    (0): Dropout(p=0.5, inplace=False)\n",
              "    (1): Linear(in_features=9216, out_features=4096, bias=True)\n",
              "    (2): ReLU(inplace=True)\n",
              "    (3): Dropout(p=0.5, inplace=False)\n",
              "    (4): Linear(in_features=4096, out_features=4096, bias=True)\n",
              "    (5): ReLU(inplace=True)\n",
              "    (6): Linear(in_features=4096, out_features=200, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 193
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " # Training Loop\n",
        " from sklearn.model_selection import LeaveOneOut\n",
        " from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, f1_score"
      ],
      "metadata": {
        "id": "nxu53lhH7rkM"
      },
      "execution_count": 194,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " # Convert the training data into a NumPy array for LOOCV\n",
        " data_array = train_data.to_numpy()"
      ],
      "metadata": {
        "id": "yLviJ0X57yVH"
      },
      "execution_count": 195,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize lists to store predictions and labels for evaluation\n",
        "all_preds, all_labels = [], []\n",
        "print(\"Starting LOOCV...\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HZ8OaTkV763T",
        "outputId": "3852ff45-d362-4d91-9ac0-242f433ebe9e"
      },
      "execution_count": 196,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting LOOCV...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " # Leave-One-Out Cross Validation implementation\n",
        " loo = LeaveOneOut()\n",
        " for train_idx, test_idx in tqdm(loo.split(data_array)):\n",
        "    # Split data into training and test sets for this fold\n",
        "    train_samples = data_array[train_idx]\n",
        "    test_sample = data_array[test_idx]\n",
        "    # Create datasets and dataloaders for the current LOOCV split\n",
        "    train_dataset = CustomDataset(pd.DataFrame(train_samples), transform=CustomDataset.transform)\n",
        "    test_dataset = CustomDataset(pd.DataFrame(test_sample), transform=CustomDataset.transform)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
        "\n",
        "     # Reinitialize the model and optimizer for each LOOCV iteration\n",
        "    model = models.alexnet(pretrained=True)\n",
        "    model.classifier[6] = nn.Linear(4096, 200)\n",
        "    model.to(device)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
        "\n",
        " # Training loop for the current fold\n",
        " model.train()\n",
        " for inputs, labels in train_loader:\n",
        "     inputs, labels = inputs.to(device), labels.to(device)\n",
        "     optimizer.zero_grad()\n",
        "     outputs = model(inputs)\n",
        "     loss = criterion(outputs, labels)\n",
        "     loss.backward()\n",
        "     optimizer.step()\n",
        "\n",
        " # Validation loop for the current fold\n",
        " model.eval()\n",
        " with torch.no_grad(): # Corrected indentation here\n",
        "     for inputs, labels in test_loader:\n",
        "         inputs, labels = inputs.to(device), labels.to(device)\n",
        "         outputs = model(inputs)\n",
        "         all_preds.append(torch.argmax(outputs, dim=1).cpu().item())\n",
        "         all_labels.append(labels.cpu().item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qo5j0g0F8D8A",
        "outputId": "b521cb76-c9ab-4b48-e2bd-13a5be9e93d6"
      },
      "execution_count": 197,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "1000it [14:50,  1.12it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " # Calculate evaluation metrics\n",
        " conf_matrix = confusion_matrix(all_labels, all_preds)  # Confusion matrix\n",
        " accuracy = accuracy_score(all_labels, all_preds)      # Accuracy score\n",
        " precision = precision_score(all_labels, all_preds, average=\"macro\", zero_division=0)  # Precision score\n",
        " f1 = f1_score(all_labels, all_preds, average=\"macro\") # F1 score"
      ],
      "metadata": {
        "id": "cSCtBwNv8Kvd"
      },
      "execution_count": 198,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " # Display evaluation results\n",
        " print(\"\\nEvaluation Results:\")\n",
        " print(f\"Confusion Matrix:\\n{conf_matrix}\")\n",
        " print(f\"Accuracy: {accuracy:.4f}\")\n",
        " print(f\"Precision: {precision:.4f}\")\n",
        " print(f\"F1-Score: {f1:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XBx2pyH68LUI",
        "outputId": "9934bff9-e100-4368-b6b0-f2a3b2618723"
      },
      "execution_count": 199,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Evaluation Results:\n",
            "Confusion Matrix:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Accuracy: 0.0000\n",
            "Precision: 0.0000\n",
            "F1-Score: 0.0000\n"
          ]
        }
      ]
    }
  ]
}